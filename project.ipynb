{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKMvTlZQX90W"
      },
      "source": [
        "# Build a Convolutional Neural Network (CNN) model to classify images from a given dataset into predefined categories/classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldvRnJR9X90Y"
      },
      "source": [
        "### Importing general dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FAfJHsBbX90Y"
      },
      "outputs": [],
      "source": [
        "#Import general dependencies\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, AveragePooling2D, Input, Add, Flatten, Dropout, MaxPool2D, MaxPooling2D\n",
        "from keras.initializers import glorot_uniform as gu\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#!git clone https://github.com/sofzcc/project-1-deep-learning-image-classification-with-cnn/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBSyKKX-X90Z"
      },
      "source": [
        "### Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d alessiocorrado99/animals10 -p /content\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = '/content/animals10.zip'\n",
        "\n",
        "# Define the directory where you want to extract the files\n",
        "extract_dir = '/content/animals10/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "XDiuMItybuPM",
        "outputId": "00525114-b2c5-4953-f98a-ae21aef2c0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/alessiocorrado99/animals10\n",
            "License(s): GPL-2.0\n",
            "animals10.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "animals_dir = '/content/animals10/raw-img'\n",
        "test_dir = '/content/animals10/test-img'  # New directory for test images\n",
        "\n",
        "# Create test_dir if it doesn't exist\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)\n",
        "\n",
        "# Move a portion of images from each class in animals_dir to test_dir\n",
        "for class_name in os.listdir(animals_dir):\n",
        "    class_dir = os.path.join(animals_dir, class_name)\n",
        "    test_class_dir = os.path.join(test_dir, class_name)\n",
        "\n",
        "    if os.path.isdir(class_dir):\n",
        "        if not os.path.exists(test_class_dir):\n",
        "            os.makedirs(test_class_dir)\n",
        "\n",
        "        # Get a list of image files in the class directory\n",
        "        file_list = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "        # Randomly select 10% of the images to move to the test directory\n",
        "        num_test_images = int(len(file_list) * 0.1)\n",
        "        test_files = random.sample(file_list, num_test_images)\n",
        "\n",
        "        for file_name in test_files:\n",
        "            shutil.move(os.path.join(class_dir, file_name), os.path.join(test_class_dir, file_name))\n",
        "\n",
        "\n",
        "# Get class names\n",
        "class_names = [class_name for class_name in os.listdir(animals_dir) if os.path.isdir(os.path.join(animals_dir, class_name))]\n"
      ],
      "metadata": {
        "id": "8qdWyTzPzRmS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lee2IwjX90a"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Preprocessing (e.g., normalization, resizing, augmentation).\n",
        "Create visualizations of some images, and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GaLYDD2YX90a",
        "outputId": "c0b2845c-0908-47c1-f351-0715b7b27e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18856 images belonging to 10 classes.\n",
            "Found 4709 images belonging to 10 classes.\n",
            "Found 4976 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create ImageDataGenerators for training and validation sets\n",
        "data_generator = ImageDataGenerator(\n",
        "    rescale=1./255.0,\n",
        "    validation_split=0.2,  # Split for training and validation\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    rotation_range=30,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Create ImageDataGenerator for test set\n",
        "test_val_data_generator = ImageDataGenerator(rescale=1./255.0)\n",
        "\n",
        "# Generate training set\n",
        "train_set = data_generator.flow_from_directory(\n",
        "    animals_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Generate validation set\n",
        "val_set = data_generator.flow_from_directory(\n",
        "    animals_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Generate test set\n",
        "test_set = test_val_data_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo4VkEd8X90b"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "Design a CNN architecture suitable for image classification.\n",
        "Include convolutional layers, pooling layers, and fully connected layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#MReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero\n",
        "#and to introduce non-linearity to the network\n",
        "#Other non linear functions such as tanh or sigmoid can also be\n",
        "#used instead of ReLU, but ReLU has been found to perform better in most situations\n",
        "\n",
        "\n",
        "#gradually reduce the spatial dimensions through the network\n",
        "# while still allowing the filters to have a sufficient receptive field in the second layer.\n",
        "\n",
        "# cnn_model = tf.keras.models.Sequential([\n",
        "#     Conv2D(64, (3, 3), activation='relu', input_shape=(200, 200, 3)'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D((2, 2)),\n",
        "#     Dropout(rate=0.15),\n",
        "\n",
        "#     Conv2D(128, (3, 3), activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D(pool_size=(2, 2)),\n",
        "#     Dropout(rate=0.20),\n",
        "\n",
        "#     Flatten(),\n",
        "#     Dense(256, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(rate=0.30),\n",
        "#     Dense(3, activation='softmax'),\n",
        "# ]\n",
        "# )\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# cnn_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='accuracy', patience = 5, restore_best_weights = True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#MReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero\n",
        "#and to introduce non-linearity to the network\n",
        "#Other non linear functions such as tanh or sigmoid can also be\n",
        "#used instead of ReLU, but ReLU has been found to perform better in most situations\n",
        "\n",
        "\n",
        "#gradually reduce the spatial dimensions through the network\n",
        "# while still allowing the filters to have a sufficient receptive field in the second layer.\n",
        "\n",
        "cnn_model = tf.keras.models.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    Dropout(rate=0.30),\n",
        "    Dense(10, activation='softmax'),\n",
        "]\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='accuracy', patience = 5, restore_best_weights = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "2lNMFPS_aBc-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "id": "WkWF2Dnlq8Ci",
        "outputId": "2313d3cc-3339-4533-9eba-30a619746cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 198, 198, 64)      1792      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 198, 198, 64)      256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 198, 198, 64)      0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 198, 198, 64)      0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 196, 196, 128)     73856     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 196, 196, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 196, 196, 128)     0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 98, 98, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 98, 98, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1229312)           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               314704128 \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 314784138 (1.17 GB)\n",
            "Trainable params: 314783242 (1.17 GB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training\n",
        "\n",
        "Train the CNN model using appropriate optimization techniques (e.g., stochastic gradient descent, Adam).\n",
        "Utilize techniques such as early stopping to prevent overfitting.\n"
      ],
      "metadata": {
        "id": "6ZqD4jU0da8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model.fit(train_set, validation_data=val_set, epochs=50, callbacks=[early_stopping, reduce_lr], verbose=2)"
      ],
      "metadata": {
        "id": "Lf91JtNCawso",
        "outputId": "d6ea2ed4-5014-414e-97f5-d081fdb2870c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CLzaUzHoHZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluate the trained model on a separate validation set.\n",
        "Compute and report metrics such as accuracy, precision, recall, and F1-score.\n",
        "Visualize the confusion matrix to understand model performance across different classes"
      ],
      "metadata": {
        "id": "_b8-cLA3de3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate validation set"
      ],
      "metadata": {
        "id": "UygJTEu0B-ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(val_set)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "g9bKiFc8l8Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(test_set)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "f-cgO_pgE1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute and report metrics such as accuracy, precision, recall, and F1-score"
      ],
      "metadata": {
        "id": "ktdVpk-BCKIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score  # Add this import statement\n",
        "\n",
        "\n",
        "#Get the true labels from both validation and test sets\n",
        "validation_true_labels = val_set.classes\n",
        "test_true_labels = test_set.classes\n",
        "\n",
        "class_names = list(val_set.class_indices.keys())\n",
        "\n",
        "\n",
        "validation_pred = cnn_model.predict(val_set)\n",
        "validation_pred_labels = np.argmax(validation_pred, axis=1)\n",
        "\n",
        "acc_val = accuracy_score(validation_true_labels, validation_pred_labels)\n",
        "\n",
        "print(f\"Validation Set - Classification report:\")\n",
        "print(classification_report(validation_true_labels, validation_pred_labels, target_names=class_names))\n",
        "print(validation_pred_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_pred = cnn_model.predict(test_set)\n",
        "test_pred_labels = np.argmax(test_pred, axis=1)\n",
        "\n",
        "acc_test = accuracy_score(test_true_labels, test_pred_labels)\n",
        "\n",
        "\n",
        "print(f\"Test set - Classification report:\")\n",
        "print(classification_report(test_true_labels, test_pred_labels, target_names=class_names))\n",
        "print(test_pred_labels)"
      ],
      "metadata": {
        "id": "C0WCiaJ8CKwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute the confusion matrix"
      ],
      "metadata": {
        "id": "XMRKUYwfCKFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Set"
      ],
      "metadata": {
        "id": "ECs7mdMn5V2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#Confusion matrix\n",
        "conf_matrix = confusion_matrix(validation_true_labels, validation_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix - Validation Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWW9ZzkiCLCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set"
      ],
      "metadata": {
        "id": "RKFWQ2hDCJ0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Confusion matrix\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iquQueLS5a7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning\n",
        "\n",
        "Evaluate the accuracy of your model on a pre-trained models like ImagNet, VGG16, Inception... (pick one an justify your choice)\n",
        "Perform transfer learning with your chosen pre-trained models i.e., you will probably try a few and choose the best one."
      ],
      "metadata": {
        "id": "6MAzqHyYdhlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing the first layer and performing transfer learning using pre-trained models"
      ],
      "metadata": {
        "id": "WCmEhYHkxjkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "\n",
        "#Load the VGG16 pre-trained model with imagenet weights without the top layer\n",
        "VGG_base = VGG16(weights='imagenet', include_top=False, input_shape=(200,200,3))\n",
        "\n",
        "\n",
        "#Load the ResNet50 pre-trained model with imagenet weights without the top layer\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(200,200,3))\n",
        "\n",
        "#Load the InceptionV3 pre-trained model with imagenet weights without the top layer\n",
        "inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(200,200,3))\n",
        "\n",
        "\n",
        "def model_creation(base_model):\n",
        "\n",
        "  #Freeze the base_model layers\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  #Add custom classification layers\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  output = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  #Create the model\n",
        "  transfer_model = Model(inputs=base_model.input, outputs=output)\n",
        "  return transfer_model\n",
        "\n",
        "VGG_model = model_creation(VGG_base)\n",
        "Resnet_model = model_creation(resnet_base)\n",
        "Inception_model = model_creation(inception_base)\n",
        "\n",
        "#Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "VGG_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Resnet_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "Inception_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_VGG = VGG_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = VGG_model.evaluate(test_set)\n",
        "print(f\"VGG Model Test accuracy: {test_acc}\")\n",
        "\n",
        "\n",
        "history_inception = Inception_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_inception , test_acc_inception  = Inception_model.evaluate(test_set)\n",
        "print(f\"InceptionV3 Model Test accuracy: {test_acc_inception }\")\n",
        "\n",
        "\n",
        "# history_resnet = Resnet_model.fit(\n",
        "#     train_set,\n",
        "#     validation_data=val_set,\n",
        "#     epochs=15,\n",
        "#     callbacks=[early_stopping, reduce_lr],\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss_resnet, test_acc_resnet = Resnet_model.evaluate(test_set)\n",
        "#print(f\"Resnet 50 Model Test accuracy: {test_acc_resnet}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "b9rM-x0MxibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose one of the previous models and fine-tune it for better results"
      ],
      "metadata": {
        "id": "1E3uZU_RxzIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some of the base model layers\n",
        "for layer in VGG_model.layers[-20:]:  # Unfreeze the last 20 layers\n",
        "    if not isinstance(layer, BatchNormalization):  # Optionally leave BatchNormalization layers frozen\n",
        "        layer.trainable = True\n",
        "\n",
        "# Recompile the model (necessary after changing layer trainability)\n",
        "VGG_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_final = VGG_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_final, test_acc_final = VGG_model.evaluate(test_set)\n",
        "print(f\"Final Model Test accuracy: {test_acc_final}\")\n"
      ],
      "metadata": {
        "id": "oFglVtnl7NQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b4VqxLo7Rv80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to disk\n",
        "\n",
        "model_dir = \"./model\"\n",
        "model_version = 1\n",
        "model_export_path = f\"{model_dir}/{model_version}\"\n",
        "\n",
        "tf.saved_model.save(\n",
        "    VGG_model,\n",
        "    export_dir=model_export_path,\n",
        ")\n",
        "\n",
        "print(f\"SavedModel files: {os.listdir(model_export_path)}\")"
      ],
      "metadata": {
        "id": "NO5ZyB5zx3N8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "history_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}