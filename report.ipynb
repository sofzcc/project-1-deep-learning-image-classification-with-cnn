{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKMvTlZQX90W"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "## 1.1 Project Overview\n",
        "\n",
        "The objective of this project is to develop a machine learning model capable of classifying images of animals into their respective categories. This classification task is crucial for applications in wildlife monitoring, automated animal detection in images, and educational tools in zoology and biodiversity.\n",
        "\n",
        "## 1.2 Dataset\n",
        "\n",
        "The dataset used for this project is the Animals10 dataset, which contains images of ten different animal classes. The dataset includes a total of 28,000 images with each class having a diverse set of images representing various poses, backgrounds, and lighting conditions. The ten animal classes are:\n",
        "\n",
        " * Dog\n",
        " * Cat\n",
        " * Horse\n",
        " * Spider\n",
        " * Butterfly\n",
        " * Chicken\n",
        " * Cow\n",
        " * Sheep\n",
        " * Squirrel\n",
        " * Elephant\n",
        "\n",
        "The images are in JPEG format and come in various dimensions, reflecting real-world scenarios.\n",
        "\n",
        "\n",
        "## 1.3 Tools and Technologies\n",
        "\n",
        "* Python\n",
        "* Python Libraries:\n",
        "    * NumPy\n",
        "    * Pandas\n",
        "    * Scikit-Learn\n",
        "    * Matplotlib\n",
        "    * Seaborn\n",
        "    * Keras\n",
        "    * PIL\n",
        "* Jupyter Notebook\n",
        "* Flask\n",
        "\n",
        "### 1.4 Loading the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "!kaggle datasets download -d alessiocorrado99/animals10 -p /content\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = '/content/animals10.zip'\n",
        "\n",
        "# Define the directory where you want to extract the files\n",
        "extract_dir = '/content/animals10/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "\n",
        "animals_dir = '/content/animals10/raw-img'\n",
        "\n",
        "# Verify that the directory exists\n",
        "if not os.path.exists(animals_dir):\n",
        "    raise ValueError(f\"Directory {animals_dir} does not exist\")\n"
      ],
      "metadata": {
        "id": "ybbkwai8hauG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldvRnJR9X90Y"
      },
      "source": [
        "## 2. Data Exploration and Preprocessing\n",
        "### 2.1. Data Exploration\n",
        "\n",
        "Initially, I explored the dataset to understand its structure and characteristics. This included:\n",
        "\n",
        "* Dataset Structure: The Animals10 dataset consists of images categorized into ten different animal classes, with each class stored in a separate directory. Each directory contains various images of the respective animal.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names\n",
        "class_names = [class_name for class_name in os.listdir(animals_dir) if os.path.isdir(os.path.join(animals_dir, class_name))]"
      ],
      "metadata": {
        "id": "fJOm0rPyh1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Class Distribution: We checked the distribution of images across different classes to ensure there is a balance. If the classes were imbalanced, this could affect the performance of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "hSXMiI76hzm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting total images per class\n",
        "image_class = {}\n",
        "image_formats = set()\n",
        "image_dimensions = []\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(animals_dir, class_name)\n",
        "    file_list = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    image_class[class_name] = len(file_list)\n",
        "\n",
        "    # Check image formats and dimensions\n",
        "    for file_name in file_list:\n",
        "        img_path = os.path.join(class_dir, file_name)\n",
        "        with Image.open(img_path) as img:\n",
        "            image_formats.add(img.format)\n",
        "            image_dimensions.append(img.size)\n",
        "\n",
        "\n",
        "# Print total images per class\n",
        "print(\"\\n\\nTotal images per class in the dataset:\")\n",
        "for class_name, count in image_class.items():\n",
        "    print(f\"{class_name} : {count}\")"
      ],
      "metadata": {
        "id": "eV8_FNpNh12q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Image Dimensions and Formats: We verified the dimensions and formats of the images to standardize preprocessing steps. The images were in JPEG format, but the dimensions varied, necessitating resizing during preprocessing."
      ],
      "metadata": {
        "id": "2RstuBFIh3Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print image formats\n",
        "print(\"\\nImage formats in the dataset:\")\n",
        "print(image_formats)\n",
        "\n",
        "# Print image dimensions\n",
        "print(\"\\nSample image dimensions:\")\n",
        "print(f\"Min dimensions: {np.min(image_dimensions, axis=0)}\")\n",
        "print(f\"Max dimensions: {np.max(image_dimensions, axis=0)}\")\n",
        "print(f\"Average dimensions: {np.mean(image_dimensions, axis=0)}\")"
      ],
      "metadata": {
        "id": "-XTkXoRkh3YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sample Visualization: We visualized random samples from each class to get an intuition about the dataset and identify any anomalies or variations in the images that might require specific preprocessing steps."
      ],
      "metadata": {
        "id": "uajf1izIh4rk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAfJHsBbX90Y"
      },
      "outputs": [],
      "source": [
        "# Plotting sample images from each class\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))\n",
        "axes = axes.flatten()\n",
        "for ax, class_name in zip(axes, class_names):\n",
        "    class_dir = os.path.join(animals_dir, class_name)\n",
        "    file_list = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "    if file_list:\n",
        "        path_sample = os.path.join(class_dir, file_list[0])  # Selecting the first file\n",
        "        img = image.load_img(path_sample, target_size=(200, 200))\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(class_name)\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        continue\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Preprocessing\n",
        "\n",
        "Data preprocessing steps included:\n",
        "\n",
        "* Handling Missing Values: There were no missing values in the dataset since all data points are images stored in directories.\n",
        "\n",
        "* Data Splitting: We split the data into training, validation, and test sets. We moved 10% of the images from each class to the test set, and further split the remaining data into training (80%) and validation (20%) sets using the ImageDataGenerator.\n",
        "\n",
        "* Image Resizing and Scaling: All images were resized to a uniform size of 200x200 pixels to ensure consistency. We also rescaled pixel values to the range [0, 1] by dividing by 255.0.\n",
        "\n",
        "* Data Augmentation: To enhance model generalization, we applied several augmentation techniques:\n",
        "\n",
        " * Horizontal Flip: Randomly flipping images horizontally.\n",
        " * Zooming: Randomly zooming into images.\n",
        " * Width and Height Shifts: Randomly shifting images horizontally and vertically.\n",
        " * Shearing: Applying random shearing transformations.\n",
        " * Brightness Adjustment: Varying the brightness of images.\n",
        " * Rotation: Randomly rotating images within a specified range."
      ],
      "metadata": {
        "id": "OcgMT97ik_EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ImageDataGenerators for training and validation sets\n",
        "data_generator = ImageDataGenerator(\n",
        "    rescale=1./255.0,\n",
        "    validation_split=0.2,  # Split for training and validation\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    rotation_range=30,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Create ImageDataGenerator for test set\n",
        "test_val_data_generator = ImageDataGenerator(rescale=1./255.0)\n",
        "\n",
        "# Generate training set\n",
        "train_set = data_generator.flow_from_directory(\n",
        "    animals_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Generate validation set\n",
        "val_set = data_generator.flow_from_directory(\n",
        "    animals_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Generate test set\n",
        "test_set = test_val_data_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=64,\n",
        "    class_mode='sparse'\n",
        ")\n"
      ],
      "metadata": {
        "id": "bh-fqIu9lHFD",
        "outputId": "3d341efa-71d3-40a8-b6f2-2bfa56629063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original dataset contains the following classes:\n",
            "ragno\n",
            "cavallo\n",
            "gallina\n",
            "pecora\n",
            "gatto\n",
            "scoiattolo\n",
            "cane\n",
            "elefante\n",
            "mucca\n",
            "farfalla\n",
            "\n",
            "\n",
            "Total images per class in the original dataset:\n",
            "ragno : 4339\n",
            "cavallo : 2361\n",
            "gallina : 2789\n",
            "pecora : 1638\n",
            "gatto : 1502\n",
            "scoiattolo : 1676\n",
            "cane : 4377\n",
            "elefante : 1302\n",
            "mucca : 1680\n",
            "farfalla : 1901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo4VkEd8X90b"
      },
      "source": [
        "## 3. Model Development\n",
        "\n",
        "### 3.1 Model Selection\n",
        "\n",
        "The first phase of the project implied creating a custom made Convolutional Neural Network architecture, that would be suitable for image classification tasks.\n",
        "\n",
        "In the first few iterations of the model, I started with a simpler structure, with less layers.\n",
        "\n",
        "At the final stage, I noticed that the model precision, f1 score and accuracy performed badly for the validation and test sets, so I introduced reLU activation.\n",
        "\n",
        "MReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero and to introduce non-linearity to the network\n",
        "Other non linear functions such as tanh or sigmoid can also be used instead of ReLU, but ReLU has been found to perform better in most situations gradually reduce the spatial dimensions through the network while still allowing the filters to have a sufficient receptive field in the second layer.\n",
        "\n",
        "The final CNN model included layers such as convolutional layers, batch normalization, ReLU activation, max pooling, and dropout to reduce overfitting. The model was trained with the Adam optimizer and sparse categorical cross-entropy loss function, achieving reasonable accuracy on both the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = tf.keras.models.Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    Dropout(rate=0.15),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(rate=0.20),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    Dropout(rate=0.30),\n",
        "    Dense(10, activation='softmax'),\n",
        "]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2lNMFPS_aBc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary\n"
      ],
      "metadata": {
        "id": "oqvD4Wv-lK4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "id": "WkWF2Dnlq8Ci",
        "outputId": "952f1443-0278-46b6-95c4-2e4ea80e3a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 198, 198, 64)      1792      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 198, 198, 64)      256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 99, 99, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 99, 99, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 97, 97, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 97, 97, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 48, 48, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 48, 48, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 294912)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               75497728  \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75575939 (288.30 MB)\n",
            "Trainable params: 75575043 (288.30 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Input Layer:\n",
        "Input shape: (200, 200, 3) indicating images of size 200x200 pixels with 3 color channels (RGB).\n",
        "\n",
        "* Convolutional Layers:\n",
        "\n",
        "The first convolutional layer has 64 filters, each with a kernel size of (3, 3) and ReLU activation.\n",
        "The second convolutional layer has 128 filters, also with a kernel size of (3, 3) and ReLU activation.\n",
        "Both convolutional layers use 'valid' padding by default.\n",
        "\n",
        "* Batch Normalization:\n",
        "\n",
        "Batch normalization layers are added after each convolutional layer. They help in normalizing the activations of the previous layer, reducing internal covariate shift, and potentially speeding up training.\n",
        "\n",
        "* Activation Layers (ReLU):\n",
        "\n",
        "ReLU activation layers are added after batch normalization. They introduce non-linearity to the model, allowing it to learn complex patterns in the data.\n",
        "\n",
        "* Pooling Layer:\n",
        "\n",
        "Max pooling with a pool size of (2, 2) is applied after the second convolutional layer. This layer reduces the spatial dimensions of the feature maps, helping in reducing computational complexity and controlling overfitting.\n",
        "\n",
        "* Dropout Layers:\n",
        "\n",
        "Dropout layers are added after the activation layers. They randomly set a fraction of input units to 0 during training, which helps prevent overfitting by forcing the network to learn redundant representations.\n",
        "\n",
        "* Flatten Layer:\n",
        "\n",
        "The Flatten layer converts the 2D feature maps into a 1D vector, preparing the data for input into the fully connected layers.\n",
        "\n",
        "* Fully Connected (Dense) Layers:\n",
        "\n",
        "There are two fully connected layers with 256 and 10 neurons respectively.\n",
        "ReLU activation functions are used in these layers, introducing non-linearity.\n",
        "The last layer uses a softmax activation function, suitable for multi-class classification tasks like yours."
      ],
      "metadata": {
        "id": "YFqETjUvldu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Model Training and Evaluation\n",
        "\n",
        "The model was compiled with the Adam optimizer, utilizing a learning rate of 0.0001, and categorical cross-entropy loss function, aiming to optimize accuracy during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZqD4jU0da8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "cnn_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-PJfRINU_Md_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping and ReduceLROnPlateau**\n",
        "\n",
        "To monitor and control the training process, two callback functions were implemented: EarlyStopping and ReduceLROnPlateau.\n",
        "\n",
        "EarlyStopping was set to monitor accuracy and halt training if there was no improvement after 5 consecutive epochs.\n",
        "\n",
        "ReduceLROnPlateau was employed to adjust the learning rate dynamically by a factor of 0.2 if no improvement in validation loss was observed after 5 epochs, with a lower limit set to 0.001.\n",
        "\n"
      ],
      "metadata": {
        "id": "f-q1HcrW_Pm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='accuracy', patience = 5, restore_best_weights = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)"
      ],
      "metadata": {
        "id": "9exy-E0b_QHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained using the fit method with the provided training set (train_set) and validated against the validation set (val_set) over 50 epochs.\n",
        "\n",
        "Subsequently, the model's performance was evaluated on both the validation and test sets. The evaluation included measuring accuracy, along with generating classification reports to gain insights into the model's performance across different classes."
      ],
      "metadata": {
        "id": "CCreQzNZ_i-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn_model.fit(train_set, validation_data=val_set, epochs=50, callbacks=[early_stopping, reduce_lr], verbose=2)"
      ],
      "metadata": {
        "id": "Lf91JtNCawso",
        "outputId": "6df35d92-d9b3-4fb7-904e-6f76682f3e6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "295/295 - 912s - loss: 1.7654 - accuracy: 0.4020 - val_loss: 2.0458 - val_accuracy: 0.3682 - lr: 1.0000e-04 - 912s/epoch - 3s/step\n",
            "Epoch 2/50\n",
            "295/295 - 929s - loss: 1.6270 - accuracy: 0.4394 - val_loss: 1.8324 - val_accuracy: 0.4334 - lr: 1.0000e-04 - 929s/epoch - 3s/step\n",
            "Epoch 3/50\n",
            "295/295 - 936s - loss: 1.5382 - accuracy: 0.4746 - val_loss: 1.7231 - val_accuracy: 0.4576 - lr: 1.0000e-04 - 936s/epoch - 3s/step\n",
            "Epoch 4/50\n",
            "295/295 - 930s - loss: 1.4667 - accuracy: 0.4947 - val_loss: 1.7240 - val_accuracy: 0.4774 - lr: 1.0000e-04 - 930s/epoch - 3s/step\n",
            "Epoch 5/50\n",
            "295/295 - 925s - loss: 1.3967 - accuracy: 0.5193 - val_loss: 1.6860 - val_accuracy: 0.4804 - lr: 1.0000e-04 - 925s/epoch - 3s/step\n",
            "Epoch 6/50\n",
            "295/295 - 932s - loss: 1.3542 - accuracy: 0.5362 - val_loss: 1.5490 - val_accuracy: 0.5300 - lr: 1.0000e-04 - 932s/epoch - 3s/step\n",
            "Epoch 7/50\n",
            "295/295 - 946s - loss: 1.3119 - accuracy: 0.5474 - val_loss: 1.5973 - val_accuracy: 0.5111 - lr: 1.0000e-04 - 946s/epoch - 3s/step\n",
            "Epoch 8/50\n",
            "295/295 - 947s - loss: 1.2750 - accuracy: 0.5630 - val_loss: 1.4960 - val_accuracy: 0.5220 - lr: 1.0000e-04 - 947s/epoch - 3s/step\n",
            "Epoch 9/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Results\n",
        "\n",
        "### 4.1 Model Performance Evaluation"
      ],
      "metadata": {
        "id": "RpIq8fQ7_lNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualizations below illustrate the training and validation loss, as well as the training and validation accuracy over the course of the training epochs."
      ],
      "metadata": {
        "id": "FayuXU3d_1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CLzaUzHoHZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UhH90wn-_1uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the model with the validation set**"
      ],
      "metadata": {
        "id": "UygJTEu0B-ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(val_set)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "g9bKiFc8l8Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the model with a test set**"
      ],
      "metadata": {
        "id": "TASnPWo9ApjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = cnn_model.evaluate(test_set)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "f-cgO_pgE1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model's accuracy, precision, recall, and F1-score**"
      ],
      "metadata": {
        "id": "ktdVpk-BCKIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score  # Add this import statement\n",
        "\n",
        "\n",
        "#Get the true labels from both validation and test sets\n",
        "validation_true_labels = val_set.classes\n",
        "test_true_labels = test_set.classes\n",
        "\n",
        "class_names = list(val_set.class_indices.keys())\n",
        "\n",
        "\n",
        "validation_pred = cnn_model.predict(val_set)\n",
        "validation_pred_labels = np.argmax(validation_pred, axis=1)\n",
        "\n",
        "acc_val = accuracy_score(validation_true_labels, validation_pred_labels)\n",
        "\n",
        "print(f\"Validation Set - Classification report:\")\n",
        "print(classification_report(validation_true_labels, validation_pred_labels, target_names=class_names))\n",
        "print(validation_pred_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_pred = cnn_model.predict(test_set)\n",
        "test_pred_labels = np.argmax(test_pred, axis=1)\n",
        "\n",
        "acc_test = accuracy_score(test_true_labels, test_pred_labels)\n",
        "\n",
        "\n",
        "print(f\"Test set - Classification report:\")\n",
        "print(classification_report(test_true_labels, test_pred_labels, target_names=class_names))\n",
        "print(test_pred_labels)"
      ],
      "metadata": {
        "id": "C0WCiaJ8CKwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Confusion matrix**"
      ],
      "metadata": {
        "id": "XMRKUYwfCKFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Set**"
      ],
      "metadata": {
        "id": "ECs7mdMn5V2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#Confusion matrix\n",
        "conf_matrix = confusion_matrix(validation_true_labels, validation_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix - Validation Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWW9ZzkiCLCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set**"
      ],
      "metadata": {
        "id": "RKFWQ2hDCJ0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Confusion matrix\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iquQueLS5a7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Transfer Learning\n",
        "\n",
        "After evaluating the performance of the initial CNN model, transfer learning was employed to improve the model's accuracy and generalization capabilities.\n",
        "\n",
        "The following pre-trained models were considered for transfer learning:\n",
        "\n",
        "* **VGG16**: VGG16 is a popular deep learning model with 16 layers, known for its simplicity and effectiveness in image classification tasks. Its architecture consists of sequential convolutional layers followed by fully connected layers.\n",
        "\n",
        "* **ResNet50**: ResNet50, a 50-layer residual network, introduces skip connections to address the vanishing gradient problem, enabling the training of deeper networks. It has shown superior performance in various image classification benchmarks.\n",
        "\n",
        "* **InceptionV3**: InceptionV3, part of the Inception family of models, uses a more complex architecture with inception modules that allow the network to learn richer feature representations by combining multiple convolutional filters."
      ],
      "metadata": {
        "id": "6MAzqHyYdhlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Removing the first layer and performing transfer learning using pre-trained models"
      ],
      "metadata": {
        "id": "WCmEhYHkxjkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I created a function to create the models according to a given pre-trained model."
      ],
      "metadata": {
        "id": "qpybTxVwDHVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_creation(base_model):\n",
        "\n",
        "  #Freeze the base_model layers\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  #Add custom classification layers\n",
        "  x = Flatten()(base_model.output)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  output = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  #Create the model\n",
        "  transfer_model = Model(inputs=base_model.input, outputs=output)\n",
        "  return transfer_model\n"
      ],
      "metadata": {
        "id": "IG77rsPLDGIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG16**\n",
        "\n",
        "Achieved good accuracy on the validation and test sets.\n",
        "Benefits from a straightforward architecture, making it easier to fine-tune and interpret."
      ],
      "metadata": {
        "id": "54A93rD-C3wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "#Load the VGG16 pre-trained model with imagenet weights without the top layer\n",
        "VGG_base = VGG16(weights='imagenet', include_top=False, input_shape=(200,200,3))\n",
        "\n",
        "VGG_model = model_creation(VGG_base)\n",
        "\n",
        "#Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "VGG_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_VGG = VGG_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = VGG_model.evaluate(test_set)\n",
        "print(f\"VGG Model Test accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "id": "ESZZ0ANWC6uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50**\n",
        "\n",
        "Not included in the final comparison due to resource constraints, but generally expected to perform well due to its deeper architecture and skip connections."
      ],
      "metadata": {
        "id": "87erk967DpSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.resnet import ResNet50\n",
        "\n",
        "#Load the ResNet50 pre-trained model with imagenet weights without the top layer\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "Resnet_model = model_creation(resnet_base)\n",
        "\n",
        "#Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "Resnet_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_resnet = Resnet_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_resnet, test_acc_resnet = Resnet_model.evaluate(test_set)\n",
        "print(f\"Resnet 50 Model Test accuracy: {test_acc_resnet}\")\n"
      ],
      "metadata": {
        "id": "8arVBAISDUVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InceptionV3**\n",
        "\n",
        "Showed competitive accuracy on the validation and test sets.\n",
        "The inception modules provide a diverse set of feature representations, potentially leading to better performance in complex datasets."
      ],
      "metadata": {
        "id": "EAMknUAoDnNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "#Load the InceptionV3 pre-trained model with imagenet weights without the top layer\n",
        "inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(200,200,3))\n",
        "\n",
        "Inception_model = model_creation(inception_base)\n",
        "\n",
        "#Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "Inception_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_inception = Inception_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_inception , test_acc_inception  = Inception_model.evaluate(test_set)\n",
        "print(f\"InceptionV3 Model Test accuracy: {test_acc_inception }\")\n"
      ],
      "metadata": {
        "id": "b9rM-x0MxibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Transfer Learning Evaluation"
      ],
      "metadata": {
        "id": "1E3uZU_RxzIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of Pre-trained Models**\n",
        "\n",
        "Each pre-trained model was fine-tuned on the Animals10 dataset, and their performances were compared based on validation accuracy and test accuracy. The key findings are summarized below:\n",
        "\n",
        "\n",
        "\n",
        "**Final Model Selection**\n",
        "The final model selected was VGG16 due to its balance of simplicity and performance. Additionally, VGG16 was further fine-tuned by unfreezing the last few layers and retraining them to adapt specifically to the Animals10 dataset. This fine-tuning process further improved the model's accuracy.\n",
        "\n",
        "* Performance: VGG16 demonstrated strong performance on the validation and test sets, indicating its ability to generalize well to new images.\n",
        "* Simplicity: The straightforward architecture of VGG16 made it easier to fine-tune and interpret compared to more complex models.\n",
        "* Resource Efficiency: VGG16 required less computational resources compared to deeper models like ResNet50, making it a practical choice for this project.\n",
        "\n",
        "Overall, VGG16 provided a good balance of accuracy, interpretability, and computational efficiency, making it the ideal choice for the animal species classification task."
      ],
      "metadata": {
        "id": "ol_2uKKlDv94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some of the base model layers\n",
        "for layer in VGG_model.layers[-20:]:  # Unfreeze the last 20 layers\n",
        "    if not isinstance(layer, BatchNormalization):  # Optionally leave BatchNormalization layers frozen\n",
        "        layer.trainable = True\n",
        "\n",
        "# Recompile the model (necessary after changing layer trainability)\n",
        "VGG_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_final = VGG_model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=15,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_final, test_acc_final = VGG_model.evaluate(test_set)\n",
        "print(f\"Final Model Test accuracy: {test_acc_final}\")\n"
      ],
      "metadata": {
        "id": "oFglVtnl7NQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b4VqxLo7Rv80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to disk\n",
        "\n",
        "model_dir = \"./model\"\n",
        "model_version = 1\n",
        "model_export_path = f\"{model_dir}/{model_version}\"\n",
        "\n",
        "tf.saved_model.save(\n",
        "    VGG_model,\n",
        "    export_dir=model_export_path,\n",
        ")\n",
        "\n",
        "print(f\"SavedModel files: {os.listdir(model_export_path)}\")"
      ],
      "metadata": {
        "id": "NO5ZyB5zx3N8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}